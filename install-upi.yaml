---
- name: dark magic
  gather_facts: false
  hosts: localhost
  vars_files:
    - "{{ playbook_dir }}/vars.yaml"
  environment:
    KUBECONFIG: /tmp/{{ clustername }}/auth/kubeconfig

  tasks:
    - include_tasks: tasks/installer-config-directory.yaml

    - name: "Check if /tmp/{{ clustername }}/metadata.json exists"
      stat:
        path: /tmp/{{ clustername }}/metadata.json
      register: stat_result

    - name: render template
      template:
        src: files/install-config.yaml.j2
        dest: /tmp/{{ clustername }}/install-config.yaml
      when: stat_result.stat.exists == False

    - name: Create the cluster ignition configs
      command: openshift-install create ignition-configs --dir=/tmp/{{ clustername }}
      args:
        creates: /tmp/{{ clustername }}/metadata.json
      when: stat_result.stat.exists == False

    - name: read master CA info
      command: cat /tmp/{{ clustername }}/master.ign
      register: result

    - set_fact:
        master_ignition: "{{ result.stdout | from_json }}"

    - include_tasks: tasks/get_infraid.yaml

#    - debug:
#        msg: "{{ master_ignition.ignition.security.tls.certificateAuthorities[0].source }}"
#
#    - debug:
#        msg: "{{ metadata_json.infraID }}"
    

    - include_tasks: tasks/bootstrap-s3-bucket.yaml

    - name: upload bootstrap.ign to s3 bucket
      aws_s3:
        bucket: "{{ infraid }}-bootstrap"
        object: /bootstrap.ign
        src: /tmp/{{ clustername }}/bootstrap.ign
        mode: "{{ state | default('put') }}"
      retries: 3
      delay: 3

    - name: Create UPI Network Elements (Route53 & LBs)
      cloudformation:
        stack_name: "{{ infraid }}-cluster-infra"
        state: present
        region: "{{ region }}"
        template: "files/cloudformation/02_cluster_infra.yaml{% if api_internal_only is sameas true %}.internalonly{% else %}.existingvpc{% endif %}"
        template_parameters:
          ClusterName: "{{ clustername }}"
          InfrastructureName: "{{ infraid }}"
          PrivateSubnets: "{{ privatesubnets}}"
          PrivateZoneId: "{{ privatezoneid }}"
          PublicSubnets: "{{ publicsubnets }}"
          HostedZoneId: "{{ publiczoneid }}"
          HostedZoneName: "{{ publiczonename }}"
          VpcId: "{{ vpcid }}"
        tags:
          clustername: "{{ clustername }}"
          infraid: "{{ infraid }}"
      register: cluster_infra_stack

    - name: Create UPI Security Elements (Security Groups & IAM)
      cloudformation:
        stack_name: "{{ infraid }}-cluster-security"
        state: present
        region: "{{ region }}"
        template: "files/cloudformation/03_cluster_security.yaml"
        template_parameters:
          InfrastructureName: "{{ infraid }}"
          PrivateSubnets: "{{ privatesubnets}}"
          VpcId: "{{ vpcid }}"
          VpcCidr: "{{ vpccidr }}"
        tags:
          clustername: "{{ clustername }}"
          infraid: "{{ infraid }}"
      register: cluster_security_stack

#    - debug:
#        msg: "{{ cluster_security_stack.stack_outputs.MasterSecurityGroupId }}"

    - name: Create UPI Bootstrap (EC2 Instance, Security Groups and IAM)
      cloudformation:
        stack_name: "{{ infraid }}-cluster-bootstrap"
        state: present
        region: "{{ region }}"
        template: "files/cloudformation/04_cluster_bootstrap.yaml{% if api_internal_only is sameas true %}.internalonly{% endif %}"
        template_parameters:
          InfrastructureName: "{{ infraid }}"
          BootstrapIgnitionLocation: s3://{{ infraid }}-bootstrap//bootstrap.ign
          VpcId: "{{ vpcid }}"
          AutoRegisterELB: "yes"
          AllowedBootstrapSshCidr: "0.0.0.0/0"
          MasterSecurityGroupId: "{{ cluster_security_stack.stack_outputs.MasterSecurityGroupId }}"
          PublicSubnet: "{{ publicsubnets.split(',')[0] }}"
          RhcosAmi: "{{ rhcos_ami }}"
          ExternalApiTargetGroupArn: "{{ cluster_infra_stack.stack_outputs.ExternalApiTargetGroupArn if api_internal_only == 'false' else omit }}"
          InternalApiTargetGroupArn: "{{ cluster_infra_stack.stack_outputs.InternalApiTargetGroupArn }}"
          InternalServiceTargetGroupArn: "{{ cluster_infra_stack.stack_outputs.InternalServiceTargetGroupArn }}"
          RegisterNlbIpTargetsLambdaArn: "{{ cluster_infra_stack.stack_outputs.RegisterNlbIpTargetsLambda }}"
        tags:
          clustername: "{{ clustername }}"
          infraid: "{{ infraid }}"

    - name: Create UPI Node Launch (EC2 master instances)
      cloudformation:
        stack_name: "{{ infraid }}-cluster-master-nodes"
        state: present
        region: "{{ region }}"
        template: "files/cloudformation/05_cluster_master_nodes.yaml{% if api_internal_only is sameas true %}.internalonly{% endif %}"
        template_parameters:
          InfrastructureName: "{{ infraid }}"
          AutoRegisterELB: "yes"
          MasterSecurityGroupId: "{{ cluster_security_stack.stack_outputs.MasterSecurityGroupId }}"
          Master0Subnet: "{{ privatesubnets.split(',')[0] }}"
          Master1Subnet: "{{ privatesubnets.split(',')[1] }}"
          Master2Subnet: "{{ privatesubnets.split(',')[2] }}"
          MasterInstanceProfileName: "{{ cluster_security_stack.stack_outputs.MasterInstanceProfile }}"
          MasterInstanceType: "{{ master_instance }}"
          RhcosAmi: "{{ rhcos_ami }}"
          ExternalApiTargetGroupArn: "{{ cluster_infra_stack.stack_outputs.ExternalApiTargetGroupArn if api_internal_only == 'false' else omit }}"
          InternalApiTargetGroupArn: "{{ cluster_infra_stack.stack_outputs.InternalApiTargetGroupArn }}"
          InternalServiceTargetGroupArn: "{{ cluster_infra_stack.stack_outputs.InternalServiceTargetGroupArn }}"
          RegisterNlbIpTargetsLambdaArn: "{{ cluster_infra_stack.stack_outputs.RegisterNlbIpTargetsLambda }}"
          AutoRegisterDNS: "yes"
          IgnitionLocation: "https://api.{{ clustername }}.{{ publiczonename }}:22623/config/master"
          PrivateHostedZoneId: "{{ privatezoneid }}"
#this is confusing but right for in some cases. in others use the privatezonename var
          PrivateHostedZoneName: "{{ clustername }}.{{ publiczonename }}"
          CertificateAuthorities: "{{ master_ignition.ignition.security.tls.certificateAuthorities[0].source }}"
        tags:
          clustername: "{{ clustername }}"
          infraid: "{{ infraid }}"
          auto_shut_bool: "{{ automatic_shutdown | default(omit) }}"

#TODO: change master ec2 instance termination protection to true and then remove old machine api objects after

    - name: tag public subnets
      command: aws ec2 create-tags --resources {{ item }} --tags Key=kubernetes.io/cluster/{{ infraid }},Value=shared
      with_items: "{{ publicsubnets.split(',') }}"

    - name: tag private subnets
      command: aws ec2 create-tags --resources {{ item }} --tags Key=kubernetes.io/role/internal-elb,Value=""
      with_items: "{{ privatesubnets.split(',') }}"

    - name: "Check if /tmp/{{ clustername }}/bootstrap-complete exists"
      stat:
        path: /tmp/{{ clustername }}/bootstrap-complete
      register: bootstrap_result

    - name: Wait until bootstrap is complete
      shell: >
        openshift-install user-provided-infrastructure bootstrap-complete --dir=/tmp/{{ clustername }} &&
        touch /tmp/{{ clustername }}/bootstrap-complete
      when: bootstrap_result.stat.exists == False

    - name: "Check if /tmp/{{ clustername }}/bootstrap-complete exists"
      stat:
        path: /tmp/{{ clustername }}/finish
      register: finish

    - name: approve master certificates
      shell: oc adm certificate approve $(oc get csr -o=jsonpath="{..metadata.name}")
      register: oc_approve
      retries: 12
      delay: 10
      until: oc_approve.rc == 0
      when: not finish.stat.exists
      ignore_errors: yes

#    - name: render machineset template
#      template:
#        src: files/machinesets.yaml.j2
#        dest: /tmp/{{ clustername }}/machineset-{{ item.value }}.yaml
#      with_dict: {a: 0, b: 1, c: 2}
#      when: finish.stat.exists == False

#    - name: import machinesets
#      command: oc -n openshift-machine-api replace -f /tmp/{{ clustername }}/machineset-{{ item }}.yaml
#      with_sequence: start=0 count=3
#      ignore_errors: yes
#      when: finish.stat.exists == False

    - name: Fix worker machineset, securitygroup, subnet, ami and instanceprofile setting
      shell: |
        oc -n openshift-machine-api patch machineset {{ infraid }}-worker-{{ region }}{{item.key}} -p '{"spec":{"template":{"spec":{"providerSpec":{"value":{"securityGroups":[{"filters":[{"name": "group-id", "values":["{{ cluster_security_stack.stack_outputs.WorkerSecurityGroupId }}"]}]}]}}}}}}' --type merge
        oc -n openshift-machine-api patch machineset {{ infraid }}-worker-{{ region }}{{item.key}} -p '{"spec":{"template":{"spec":{"providerSpec":{"value":{"subnet":{"filters":[{"name": "subnet-id", "values":["{{ privatesubnets.split(',')[item.value | int] }}"]}]}}}}}}}' --type merge
        oc -n openshift-machine-api patch machineset {{ infraid }}-worker-{{ region }}{{item.key}} -p '{"spec":{"template":{"spec":{"providerSpec":{"value":{"iamInstanceProfile":{"id": "{{ cluster_security_stack.stack_outputs.WorkerInstanceProfile }}"}}}}}}}' --type merge
        oc -n openshift-machine-api patch machineset {{ infraid }}-worker-{{ region }}{{item.key}} -p '{"spec":{"template":{"spec":{"providerSpec":{"value":{"ami":{"id": "{{ rhcos_ami }}"}}}}}}}' --type merge
      with_dict: {a: 0, b: 1, c: 2}
      when: finish.stat.exists == False

    - name: delete broken machines
      command: oc -n openshift-machine-api delete machine -l machine.openshift.io/cluster-api-machine-role=worker
      when: finish.stat.exists == False

    - name: Fix master machine loadbalancer, securitygroup, subnet, ami and instanceprofile setting
      shell: |

        oc -n openshift-machine-api patch machine {{ infraid }}-master-{{ item }} -p '{"spec":{"providerSpec":{"value":{"loadBalancers":[{"name": "{{ cluster_infra_stack.stack_outputs.InternalApiLoadBalancerName.split('/')[1] }}", "type": "network"}{% if api_internal_only != true %},{"name": "{{ cluster_infra_stack.stack_outputs.ExternalApiLoadBalancerName.split('/')[1] }}", "type": "network"}{% endif %}]}}}}' --type merge
        oc -n openshift-machine-api patch machine {{ infraid }}-master-{{ item }} -p '{"spec":{"providerSpec":{"value":{"securityGroups":[{"filters":[{"name": "group-id", "values":["{{ cluster_security_stack.stack_outputs.MasterSecurityGroupId }}"]}]}]}}}}' --type merge
        oc -n openshift-machine-api patch machine {{ infraid }}-master-{{ item }} -p '{"spec":{"providerSpec":{"value":{"subnet":{"filters":[{"name": "subnet-id", "values":["{{ privatesubnets.split(',')[item | int] }}"]}]}}}}}' --type merge
        oc -n openshift-machine-api patch machine {{ infraid }}-master-{{ item }} -p '{"spec":{"providerSpec":{"value":{"iamInstanceProfile":{"id": "{{ cluster_security_stack.stack_outputs.MasterInstanceProfile }}"}}}}}' --type merge
        oc -n openshift-machine-api patch machine {{ infraid }}-master-{{ item }} -p '{"spec":{"providerSpec":{"value":{"ami":{"id": "{{ rhcos_ami }}"}}}}}' --type merge
#        oc -n openshift-machine-api patch machine {{ infraid }}-master-{{ item }} -p '{"spec":{"providerSpec":{"value":{"loadBalancers":[{"name": "{{ cluster_infra_stack.stack_outputs.InternalApiLoadBalancerName.split('/')[1] }}", "type": "network"},{"name": "{{ cluster_infra_stack.stack_outputs.ExternalApiLoadBalancerName.split('/')[1] }}", "type": "network"}]}}}}' --type merge
#
#       oc -n openshift-machine-api patch machine {{ infraid }}-master-{{ item }} -p '{"spec":{"providerSpec":{"value":{"subnet":[{"filters":[{"name": "subnet-id", "values":["{{ privatesubnets.split(',')[item | int] }}"]}]}]}}}}' --type merge
      with_sequence: start=0 count=3


    - name: render DNS settings
      template:
        src: files/dnses.yaml.j2
        dest: /tmp/{{ clustername }}/dnses.yaml
      when: finish.stat.exists == False

    - name: fix dns settings
      command: oc replace -f /tmp/{{ clustername }}/dnses.yaml
      when: finish.stat.exists == False
        
    - name: workaround for missing kubeadmin-password file
      copy:
        dest: "/tmp/{{ clustername }}/auth/kubeadmin-password"
        content: "REDACTED, use kubeconfig instead"

    - name: Wait for ingress controller
      command: oc get -n openshift-ingress-operator ingresscontroller/default
      register: ingress_controller
      until: ingress_controller.rc == 0
      retries: 60
      delay: 10

    - name: Scale ingress controller replicas
      command: oc patch -n openshift-ingress-operator -p '{"spec":{"replicas":6}}' --type merge ingresscontroller/default
      when: finish.stat.exists == False

    - name: Wait until install is complete
      shell: >
        openshift-install user-provided-infrastructure finish --dir=/tmp/{{ clustername }} &&
        touch /tmp/{{ clustername }}/finish
      when: finish.stat.exists == False

    - name: Terminate bootstrap resources
      block:
        - name: Terminate bootstrap CloudFormation stack
          cloudformation:
            stack_name: "{{ infraid }}-cluster-bootstrap"
            region: "{{ region }}"
            state: absent

        - include_tasks: tasks/bootstrap-s3-bucket.yaml
          vars:
            state: absent
      when: boostrap_cleanup
...
