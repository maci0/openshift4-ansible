---
- name: dark magic
  gather_facts: false
  hosts: localhost
  vars_files:
    - "{{ playbook_dir }}/vars.yaml"

  tasks:
    - include_tasks: tasks/installer-config-directory.yaml

    - name: "Check if /tmp/{{ clustername }}/metadata.json exists"
      stat:
        path: /tmp/{{ clustername }}/metadata.json
      register: stat_result

    - name: render template
      template:
        src: files/install-config.yaml.j2
        dest: /tmp/{{ clustername }}/install-config.yaml
      when: stat_result.stat.exists == False

    - name: Create the cluster ignition configs
      command: openshift-install create ignition-configs --dir=/tmp/{{ clustername }}
      args:
        creates: /tmp/{{ clustername }}/metadata.json
      when: stat_result.stat.exists == False

    - name: read master CA info
      command: cat /tmp/{{ clustername }}/master.ign
      register: result

    - set_fact:
        master_ignition: "{{ result.stdout | from_json }}"

    - include_tasks: tasks/get_infraid.yaml

#    - debug:
#        msg: "{{ master_ignition.ignition.security.tls.certificateAuthorities[0].source }}"
#
#    - debug:
#        msg: "{{ metadata_json.infraID }}"
    

    - include_tasks: tasks/bootstrap-s3-bucket.yaml

    - name: upload bootstrap.ign to s3 bucket
      aws_s3:
        bucket: "{{ infraid }}-bootstrap"
        object: /bootstrap.ign
        src: /tmp/{{ clustername }}/bootstrap.ign
        mode: "{{ state | default('put') }}"
      retries: 3
      delay: 3

    - name: Create UPI Network Elements (Route53 & LBs)
      cloudformation:
        stack_name: "{{ infraid }}-cluster-infra"
        state: present
        region: "{{ region }}"
        template: "files/cloudformation/02_cluster_infra.yaml.existingvpc"
        template_parameters:
          ClusterName: "{{ clustername }}"
          InfrastructureName: "{{ infraid }}"
          PrivateSubnets: "{{ privatesubnets}}"
          PrivateZoneId: "{{ privatezoneid }}"
          PublicSubnets: "{{ publicsubnets }}"
          HostedZoneId: "{{ publiczoneid }}"
          HostedZoneName: "{{ publiczonename }}"
          VpcId: "{{ vpcid }}"
        tags:
          clustername: "{{ clustername }}"
          infraid: "{{ infraid }}"
      register: cluster_infra_stack

#    - debug:
#        msg: "{{ cluster_infra_stack.stack_outputs}}"

    - name: Create UPI Security Elements (Security Groups & IAM)
      cloudformation:
        stack_name: "{{ infraid }}-cluster-security"
        state: present
        region: "{{ region }}"
        template: "files/cloudformation/03_cluster_security.yaml"
        template_parameters:
          InfrastructureName: "{{ infraid }}"
          PrivateSubnets: "{{ privatesubnets}}"
          VpcId: "{{ vpcid }}"
          VpcCidr: "{{ vpccidr }}"
        tags:
          clustername: "{{ clustername }}"
          infraid: "{{ infraid }}"
      register: cluster_security_stack

#    - debug:
#        msg: "{{ cluster_security_stack.stack_outputs.MasterSecurityGroupId }}"

    - name: Create UPI Bootstrap (EC2 Instance, Security Groups and IAM)
      cloudformation:
        stack_name: "{{ infraid }}-cluster-bootstrap"
        state: present
        region: "{{ region }}"
        template: "files/cloudformation/04_cluster_bootstrap.yaml"
        template_parameters:
          InfrastructureName: "{{ infraid }}"
          BootstrapIgnitionLocation: s3://{{ infraid }}-bootstrap//bootstrap.ign
          VpcId: "{{ vpcid }}"
          AutoRegisterELB: "yes"
          AllowedBootstrapSshCidr: "0.0.0.0/0"
          MasterSecurityGroupId: "{{ cluster_security_stack.stack_outputs.MasterSecurityGroupId }}"
          PublicSubnet: "{{ publicsubnets.split(',')[0] }}"
          RhcosAmi: "{{ rhcos_ami }}"
          ExternalApiTargetGroupArn: "{{ cluster_infra_stack.stack_outputs.ExternalApiTargetGroupArn }}"
          InternalApiTargetGroupArn: "{{ cluster_infra_stack.stack_outputs.InternalApiTargetGroupArn }}"
          InternalServiceTargetGroupArn: "{{ cluster_infra_stack.stack_outputs.InternalServiceTargetGroupArn }}"
          RegisterNlbIpTargetsLambdaArn: "{{ cluster_infra_stack.stack_outputs.RegisterNlbIpTargetsLambda }}"
        tags:
          clustername: "{{ clustername }}"
          infraid: "{{ infraid }}"

    - name: Create UPI Node Launch (EC2 master instances)
      cloudformation:
        stack_name: "{{ infraid }}-cluster-master-nodes"
        state: present
        region: "{{ region }}"
        template: "files/cloudformation/05_cluster_master_nodes.yaml"
        template_parameters:
          InfrastructureName: "{{ infraid }}"
          AutoRegisterELB: "yes"
          MasterSecurityGroupId: "{{ cluster_security_stack.stack_outputs.MasterSecurityGroupId }}"
          Master0Subnet: "{{ privatesubnets.split(',')[0] }}"
          Master1Subnet: "{{ privatesubnets.split(',')[1] }}"
          Master2Subnet: "{{ privatesubnets.split(',')[2] }}"
          MasterInstanceProfileName: "{{ cluster_security_stack.stack_outputs.MasterInstanceProfile }}"
          MasterInstanceType: "m4.2xlarge"
          RhcosAmi: "{{ rhcos_ami }}"
          ExternalApiTargetGroupArn: "{{ cluster_infra_stack.stack_outputs.ExternalApiTargetGroupArn }}"
          InternalApiTargetGroupArn: "{{ cluster_infra_stack.stack_outputs.InternalApiTargetGroupArn }}"
          InternalServiceTargetGroupArn: "{{ cluster_infra_stack.stack_outputs.InternalServiceTargetGroupArn }}"
          RegisterNlbIpTargetsLambdaArn: "{{ cluster_infra_stack.stack_outputs.RegisterNlbIpTargetsLambda }}"
          AutoRegisterDNS: "yes"
          IgnitionLocation: "https://api.{{ clustername }}.{{ publiczonename }}:22623/config/master"
          PrivateHostedZoneId: "{{ privatezoneid }}"
#this is confusing but right for in some cases. in others use the privatezonename var
          PrivateHostedZoneName: "{{ clustername }}.{{ publiczonename }}"
          CertificateAuthorities: "{{ master_ignition.ignition.security.tls.certificateAuthorities[0].source }}"
        tags:
          clustername: "{{ clustername }}"
          infraid: "{{ infraid }}"

#TODO: change master ec2 instance termination protection to true and then remove old machine api objects after

    - name: tag public subnets
      shell: |
        aws ec2 create-tags --resources "{{ publicsubnets.split(',')[0] }}" --tags Key=kubernetes.io/cluster/{{ infraid }},Value=shared
        aws ec2 create-tags --resources "{{ publicsubnets.split(',')[1] }}" --tags Key=kubernetes.io/cluster/{{ infraid }},Value=shared
        aws ec2 create-tags --resources "{{ publicsubnets.split(',')[2] }}" --tags Key=kubernetes.io/cluster/{{ infraid }},Value=shared
      #with_sequence: start=0 count=3

    - name: tag private subnets
      shell: |
        aws ec2 create-tags --resources "{{ privatesubnets.split(',')[0] }}" --tags Key=kubernetes.io/role/internal-elb,Value=""
        aws ec2 create-tags --resources "{{ privatesubnets.split(',')[1] }}" --tags Key=kubernetes.io/role/internal-elb,Value=""
        aws ec2 create-tags --resources "{{ privatesubnets.split(',')[2] }}" --tags Key=kubernetes.io/role/internal-elb,Value=""
      #with_sequence: start=0 count=3

    - name: "Check if /tmp/{{ clustername }}/bootstrap-complete exists"
      stat:
        path: /tmp/{{ clustername }}/bootstrap-complete
      register: bootstrap_result

    - name: Wait until bootstrap is complete
      shell: |
        set -e
        openshift-install user-provided-infrastructure bootstrap-complete --dir=/tmp/{{ clustername }}
        touch /tmp/{{ clustername }}/bootstrap-complete
      when: bootstrap_result.stat.exists == False

    - name: "Check if /tmp/{{ clustername }}/bootstrap-complete exists"
      stat:
        path: /tmp/{{ clustername }}/finish
      register: finish

    - name: approve master certificates
      shell: |
        export KUBECONFIG=/tmp/{{ clustername }}/auth/kubeconfig
        oc adm certificate approve $(oc get csr -o=jsonpath="{..metadata.name}")
      ignore_errors: yes
      when: finish.stat.exists == False

    - name: render machineset template
      template:
        src: files/machinesets.yaml.j2
        dest: /tmp/{{ clustername }}/machineset-{{ item.value }}.yaml
      with_dict: {a: 0, b: 1, c: 2}
      when: finish.stat.exists == False

    - name: import machinesets
      shell: |
        export KUBECONFIG=/tmp/{{ clustername }}/auth/kubeconfig
        oc -n openshift-machine-api replace -f /tmp/{{ clustername }}/machineset-{{ item }}.yaml
      with_sequence: start=0 count=3
      ignore_errors: yes
      when: finish.stat.exists == False

#todo only do this on first install
    - name: delete broken machines
      shell: |
        export KUBECONFIG=/tmp/{{ clustername }}/auth/kubeconfig
        oc -n openshift-machine-api delete machine    -l machine.openshift.io/cluster-api-machine-role=worker
      when: finish.stat.exists == False

    - name: render DNS settings
      template:
        src: files/dnses.yaml.j2
        dest: /tmp/{{ clustername }}/dnses.yaml
      when: finish.stat.exists == False

    - name: fix dns settings
      shell: |
        export KUBECONFIG=/tmp/{{ clustername }}/auth/kubeconfig
        oc replace -f /tmp/{{ clustername }}/dnses.yaml
      when: finish.stat.exists == False
        
    - name: workaround for missing kubeadmin-password file
      copy:
        dest: "/tmp/{{ clustername }}/auth/kubeadmin-password"
        content: "REDACTED, use kubeconfig instead"

    - name: Scale ingress controller replicas
      command: oc -n openshift-ingress-operator patch ingresscontroller/default -p '{"spec":{"replicas":6}}' --type merge
      environment:
        KUBECONFIG: /tmp/{{ clustername }}/auth/kubeconfig
      when: finish.stat.exists == False

    - name: Wait until install is complete
      shell: |
        openshift-install user-provided-infrastructure finish --dir=/tmp/{{ clustername }}
        touch /tmp/{{ clustername }}/finish
      when: finish.stat.exists == False

...
